# Pipeline Flow â€” End-to-End Documentation

## Overview

This document explains how the **Keyframe Extractor** and **Landmark Extractor** work together as a complete pipeline to transform a raw sign language video into a machine-learning-ready `.npy` file.

The pipeline is designed for **WLASL (Word-Level American Sign Language) recognition** â€” it produces training data in the exact format expected by the recognition model.

---

## Pipeline Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Video   â”‚â”€â”€â”€â”€â–¶â”‚   Keyframe     â”‚â”€â”€â”€â”€â–¶â”‚   Landmark      â”‚â”€â”€â”€â”€â–¶â”‚  .npy    â”‚
â”‚   (.mp4)  â”‚     â”‚   Extraction   â”‚     â”‚   Extraction    â”‚     â”‚  File    â”‚
â”‚           â”‚     â”‚  (30 frames)   â”‚     â”‚  (258 features) â”‚     â”‚(30, 258) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â–²                       â–²
                   â”‚                       â”‚
              Choose from 9           Choose: Pose,
              algorithms              Hands, or Both
                                      + Normalize?
                                      + Localize?
```

---

## Step-by-Step Flow

### Step 1: Video Upload

The user uploads a sign language video file (`.mp4`, `.mov`, or `.avi`) through the Streamlit web interface.

**What happens internally:**
1. The uploaded file is written to a temporary file on disk (required by OpenCV).
2. `video_utils.get_frames_from_video()` reads all frames from the video.
3. Each frame is converted from BGR (OpenCV's format) to RGB.
4. A preview of the middle frame is displayed in the UI.

**Output:** A Python list of RGB numpy arrays, one per frame (e.g., 150 frames of shape `(480, 640, 3)`).

---

### Step 2: Keyframe Extraction

The user selects one of 9 algorithms and clicks **"Extract Frames"**.

**What happens internally:**
1. The selected algorithm receives the full frame list and the target count (default: 30).
2. The algorithm analyzes the frames (motion, optical flow, keypoints, etc.) and selects the most informative ones.
3. The selected frames and their original indices are stored in session state.

**Output:** Two lists:
- `extracted_frames` â€” 30 RGB images (the keyframes)
- `extracted_indices` â€” 30 integers (which frames from the original video were selected)

**Displayed in UI:** A grid of keyframe thumbnails labeled with their original frame numbers.

See [Keyframe Extractor Documentation](keyframe_extractor.md) for algorithm details.

---

### Step 3: Landmark Extraction

The user selects a landmark method, optionally enables Normalization/Localization, and clicks **"ğŸ” Extract Landmarks"**.

**What happens internally:**
1. MediaPipe model files are downloaded (first time only) to `.models/` directory.
2. **Raw extraction pass:** Each keyframe is processed by MediaPipe PoseLandmarker and/or HandLandmarker. Per frame:
   - 33 pose landmarks Ã— 4 values = 132 floats
   - 21 left hand landmarks Ã— 3 values = 63 floats
   - 21 right hand landmarks Ã— 3 values = 63 floats
   - Missing landmarks (e.g., hand not visible) are filled with zeros.
3. The raw landmarks are padded/truncated to exactly the target frame count (30 by default).
4. Raw landmarks are saved in session state for **visualization**.
5. **Transformation pass** (if normalization or localization is enabled):
   - Landmarks are extracted again.
   - Localization centers all coordinates relative to the mid-hip point.
   - Normalization scales all values to [0, 1] range.
   - Transformed landmarks are saved separately for **data export**.

**Output:** Two numpy arrays, both of shape `(30, 258)`:
- `raw_landmarks` â€” Untransformed, used for visualization.
- `extracted_landmarks` â€” Optionally normalized/localized, used for data preview and `.npy` export.

**Displayed in UI:**
- Summary metrics (shape, non-zero frame count, value range).
- Expandable data table showing the full 30Ã—258 matrix.

See [Landmark Extractor Documentation](landmark_extractor.md) for feature layout and transformation details.

---

### Step 4: Landmark Visualization

After extraction, the landmarks are automatically drawn on the keyframe images.

**What happens internally:**
1. For each keyframe, the corresponding row from `raw_landmarks` is reshaped:
   - Pose: 33 landmarks with (x, y, z, visibility)
   - Left hand: 21 landmarks with (x, y, z)
   - Right hand: 21 landmarks with (x, y, z)
2. The (x, y) coordinates (in [0, 1] range) are scaled to pixel coordinates.
3. Skeleton connections and dots are drawn on a copy of the frame using OpenCV.

**Color coding:**
- ğŸŸ¢ **Green** â€” Pose skeleton (body joints and connections)
- ğŸŸ  **Orange** â€” Left hand landmarks and finger connections
- ğŸ”µ **Blue** â€” Right hand landmarks and finger connections

**Why raw landmarks are used for visualization:**  
Normalized/localized coordinates don't map to valid pixel positions (they can be negative or re-scaled), so the visualization always uses the original coordinates.

---

### Step 5: Save as .npy

The user clicks **"ğŸ’¾ Save Landmarks as .npy"**.

**What happens internally:**
1. A folder picker dialog opens (tkinter).
2. The `extracted_landmarks` array (shape `(30, 258)`, with any normalization/localization applied) is saved as `{video_name}.npy`.

**Output:** A single `.npy` file that can be loaded directly for model training:
```python
import numpy as np
data = np.load("video_name.npy")
print(data.shape)  # (30, 258)
```

---

### Optional: Save Frames as Images

At any point after keyframe extraction, the user can also click **"ğŸ’¾ Save Frames to Folder"** to save the raw keyframe images as `.jpg` files for manual inspection or other uses.

---

## Data Flow Diagram

```
Video File (.mp4)
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ video_utils.get_frames_from_video â”‚
â”‚ â”€ OpenCV reads all frames         â”‚
â”‚ â”€ BGR â†’ RGB conversion            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚ List of RGB frames
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ algorithms.py (chosen algorithm)  â”‚
â”‚ â”€ Analyzes motion/features        â”‚
â”‚ â”€ Selects top N frames            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚ 30 keyframes + indices
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ landmark_extractor.py             â”‚
â”‚ â”€ MediaPipe Pose + Hands          â”‚
â”‚ â”€ 258 features per frame          â”‚
â”‚ â”€ Optional: normalize/localize    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚              â”‚
  raw landmarks   transformed landmarks
       â”‚              â”‚
       â–¼              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Visualize â”‚  â”‚ Data Preview â”‚
â”‚ on frames â”‚  â”‚ & .npy Save  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## File Structure

```
sem-5-ml-project/
â”œâ”€â”€ keyframe_extractor/
â”‚   â”œâ”€â”€ app.py                  â† Streamlit UI (thin orchestration layer)
â”‚   â”œâ”€â”€ video_utils.py          â† Video I/O
â”‚   â”œâ”€â”€ algorithms.py           â† 9 keyframe extraction algorithms
â”‚   â”œâ”€â”€ file_utils.py           â† Frame saving (tkinter folder picker)
â”‚   â”œâ”€â”€ landmark_extractor.py   â† Landmark extraction + visualization
â”‚   â”œâ”€â”€ data_exporter.py        â† Numpy conversion + .npy saving
â”‚   â””â”€â”€ .models/                â† Auto-downloaded MediaPipe model files
â”œâ”€â”€ model/
â”‚   â””â”€â”€ WLASL_recognition_using_Action_Detection.ipynb  â† Training notebook
â”œâ”€â”€ doc/
â”‚   â”œâ”€â”€ keyframe_extractor.md   â† This documentation
â”‚   â”œâ”€â”€ landmark_extractor.md
â”‚   â””â”€â”€ pipeline_flow.md
â”œâ”€â”€ main.py
â”œâ”€â”€ pyproject.toml
â””â”€â”€ uv.lock
```

---

## How to Run

```bash
cd h:\Projects\sem-5-ml-project
uv run streamlit run keyframe_extractor/app.py
```

This starts a local web server (default: http://localhost:8501) with the full pipeline UI.

---

## Configuration & Dependencies

**Package manager:** `uv` (with `pyproject.toml`)

**Dependencies:**
| Package | Version | Purpose |
|---|---|---|
| `streamlit` | â‰¥1.54.0 | Web UI framework |
| `opencv-python` | â‰¥4.13.0.92 | Video I/O, image processing, optical flow |
| `numpy` | â‰¥2.4.2 | Array operations, `.npy` saving |
| `mediapipe` | â‰¥0.10.32 | Pose and hand landmark detection |

**Runtime requirements:**
- Python â‰¥3.13
- Internet connection (first run only, to download MediaPipe model files)
- Webcam not required (works with video files only)

---

## Relationship to the Model Notebook

The recognition model ([WLASL_recognition_using_Action_Detection.ipynb](../model/WLASL_recognition_using_Action_Detection.ipynb)) expects:

1. **Input:** `.npy` files with shape `(30, 258)` â€” exactly what this pipeline produces.
2. **Directory structure:** One folder per word/class, containing multiple `.npy` files (one per video).
3. **Feature layout:** `[Pose_132 | Left_Hand_63 | Right_Hand_63]` â€” matches our extraction format.

The notebook handles data augmentation, model training (LSTM-based), and evaluation. This pipeline is the **data preparation step** that comes before training.

```
This Pipeline                          Model Notebook
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                          â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Video â†’ Keyframes â†’ Landmarks â†’ .npy   â†’  Load .npy â†’ Train LSTM â†’ Predict
```
